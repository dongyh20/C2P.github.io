<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning">
  <meta name="keywords" content="Point Cloud Sequence, Self-supervised learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/zhuoyang20">Zhuoyang Zhang*</a><sup>1</sup>,</span>
<!--               Xueyi Liu<sup>1</sup>,</span> -->
            <span class="author-block">
              <a href="https://github.com/dongyh20">Yuhao Dong*</a><sup>1</sup>,</span>
<!--               Xiaomeng Xu<sup>1</sup>,</span> -->
            <span class="author-block">
              <a href="https://github.com/hoi4d">Yunze Liu</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ericyi.github.io">Li Yi</a><sup>1,2,3</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Qi Zhi Institute,</span> 
            <span class="author-block"><sup>3</sup>Shanghai Artificial Intelligence Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2212.05330.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2212.05330"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <!-- https://youtu.be/Z0wqKLgf5ME -->
                <a href="https://youtu.be/_4C7vmLN0nM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/dongyh20/C2P"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop height="100%">
        <source src="figs/teaser_2.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <!-- <span class="dnerf"></span>  -->
        C2P <span style="color: orange; font-weight:bold">proposes a new 4D self-supervised pre-training method to achieve the synergy of geometry and motion</span> for point cloud representation learning.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent work on 4D point cloud sequences has attracted a lot of attention. 
            However, obtaining exhaustively labeled 4D datasets is often very expensive and laborious, 
            so it is especially important to investigate how to utilize raw unlabeled data. 
            However, most existing self-supervised point cloud representation learning methods 
            only consider geometry from a static snapshot omitting the fact that 
            sequential observations of dynamic scenes could reveal more comprehensive geometric details. 
            And the video representation learning frameworks mostly model motion as image space flows, 
            let alone being 3D-geometric-aware. 
            To overcome such issues, <span style="font-weight:bold">this paper proposes a new 4D self-supervised pre-training method called Complete-to-Partial 4D Distillation.</span> 
            Our key idea is to formulate 4D self-supervised representation learning as a teacher-student knowledge distillation framework 
            and let the student learn useful 4D representations with the guidance of the teacher. 
            Experiments show that this approach significantly outperforms previous pre-training approaches 
            on a wide range of 4D point cloud sequence understanding tasks including indoor and outdoor scenarios.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">

          <iframe src="https://www.youtube.com/embed/_4C7vmLN0nM"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

        </div>
        
        

      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<!-- Problem overview -->
<section class="section">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Problem Overview</h2>
    <div class="columns">
      <div class="column">
        <p>AutoGPart searches for intermediate supervisions for a generalizable 3D part segmentation network. By training the network with searched features encoding correct part cues, the network could perform better when parsing an instance from a novel distribution. </p>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-three-fifths">
        <!-- <video autoplay muted loop width="100%">
          <source src="figs/groupvit_overview.mp4" type="video/mp4">
        </video> -->
        <img src="figs/overview-15.pdf" alt="">
      </div>
    </div>
    
    <h2 class="title is-3">Method Overview</h3>
    <p> AutoGPart builds an intermediate supervision space based on prior knowledge of 3D segmentation tasks. The space contains all operations to generate supervision features from input geometry features and ground-truth labels. Then, we optimize the supervision space to fit it to a given part segmentation network via a ``propose, evaluate, and update'' approach. In each update cycle, an operation is first sampled to generate supervision features for each point in the shape. Then, it is evaluated by training the network together with the task-related supervision and the intermediate supervision. Finally, a reward value indicating the network's performance evaluated under a cross-validation process is used to update the intermediate supervision space. After that, a greedy search-like approach is performed to extract supervision features from the optimized space for further use.</p>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <video autoplay muted loop playsinline controls>
          <source src="figs/method.mp4" type="video/mp4">
        </video>
      </div>
    </div>

  </div>
</section>


<section>
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Searched Intermediate Supervisions</h2>
      <div class="columns">
        <div class="column">
          <p>
            We plot an intermediate supervision feature searched by AutoGPart for the Primitive Fitting task on shapes from training domains and test domains. Compared to ground-truth segmentations (as well as their possible variations), features searched by our strategy present some patterns more friendly for a network to learn (e.g. discriminative across different parts, while changes continuously in a single part).
          </p>
        </div>
      </div>

      <h3 class="title is-4">Primitive Fitting</h3>
        <video autoplay muted loop playsinline width="100%">
          <source src="figs/website-source-searched-features.mp4" type="video/mp4">
        </video>

      <details>
        <summary class="title is-6 button">Click here for more examples on Primitive Fitting.</summary>
        <video autoplay muted loop playsinline width="100%">
          <source src="figs/website-source-more-example-prim.mp4" type="video/mp4">
        </video>
      </details>

      <h3 class="title is-4">Mobility-based Part Segmentation</h3>
        <video autoplay muted loop playsinline width="100%">
          <source src="figs/website-source-searched-features-motion.mp4" type="video/mp4">
        </video>

        <details>
          <summary class="title is-6 button">Click here for more examples on Mobility-based Part Segmentation.</summary>
          <video autoplay muted loop playsinline width="100%">
            <source src="figs/website-source-more-example-motion.mp4" type="video/mp4">
          </video>
        </details>

        <h3 class="title is-4">Semantic-based Part Segmentation</h3>
        <video autoplay muted loop playsinline width="100%">
          <source src="figs/website-source-searched-features-inst.mp4" type="video/mp4">
        </video>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Results</h2>
    <div class="columns">
      <div class="column">
        <p>We evaluate AutoGPart on three part segmentation tasks, namely Mobility-based Part Segmentation, Primitive Fitting, and Semantic-based Part Segmentation. The generalizability of part segmentation networks could be improved by when trained together with intermediate supervisions searched by our method. 
           <!-- GroupViT on Pascal VOC, Pascal Context, and COCO datasets. GroupViT is not trained on any semantic segmentation annotation, and yet could zero-shot transfer to semantic segmentation classes of any dataset without fine-tuning.  -->
          </p>
      </div>
    </div>
    <video autoplay muted loop playsinline width="100%">
      <source src="figs/website-source-seg-res-all.mp4" type="video/mp4">
    </video>
  </div>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2212.05330.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/dongyh20/C2P" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The template is borrowed from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
